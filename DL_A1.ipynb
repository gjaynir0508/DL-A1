{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbXAVaRI4EfS"
      },
      "source": [
        "# DL Assignment 1 Case Study - weight Initialization Techniques\n",
        "\n",
        "Submitted by:\n",
        "\n",
        "|   | Name | Roll No. |\n",
        "|---|---|---|\n",
        "| 1 | C. Rithesh Reddy | 160122771034 |\n",
        "| 2 | G. Jayanth       | 160122771041 |\n",
        "| 3 | J. Pavan Kumar   | 160122771045 |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries and modules"
      ],
      "metadata": {
        "id": "kHbu8VSgvMV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqd-dRuD4ERo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Normalization\n",
        "from tensorflow.keras.initializers import Zeros, RandomNormal, GlorotUniform, HeNormal, LecunNormal, Orthogonal\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
        "from tensorflow.keras.datasets import california_housing\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading datasets"
      ],
      "metadata": {
        "id": "DytnJ1SzvQIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB68eTal4HhW"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "def load_dataset(name):\n",
        "    if name in [\"MNIST\", \"Fashion-MNIST\", \"CIFAR-10\", \"CIFAR-100\"]:\n",
        "        datasets = {\n",
        "            \"MNIST\": mnist.load_data(),\n",
        "            \"Fashion-MNIST\": fashion_mnist.load_data(),\n",
        "            \"CIFAR-10\": cifar10.load_data(),\n",
        "            \"CIFAR-100\": cifar100.load_data()\n",
        "        }\n",
        "        (x_train, y_train), (x_test, y_test) = datasets[name]\n",
        "        x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data\n",
        "        return (x_train, y_train), (x_test, y_test)\n",
        "    elif name == \"California Housing\":\n",
        "        (x_train, y_train), (x_test, y_test) = california_housing.load_data()\n",
        "        normalizer = Normalization()\n",
        "        normalizer.adapt(x_train)\n",
        "        return (normalizer(x_train), y_train), (normalizer(x_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the weight initializers"
      ],
      "metadata": {
        "id": "lxhIN2GpvSQL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq5Esfhk4KZZ"
      },
      "outputs": [],
      "source": [
        "# Different weight initializations\n",
        "initializers = {\n",
        "    \"Zero Initialization\": Zeros(),\n",
        "    \"Random Initialization\": RandomNormal(mean=0.0, stddev=0.05),\n",
        "    \"Xavier Initialization\": GlorotUniform(),\n",
        "    \"He Initialization\": HeNormal(),\n",
        "    \"Lecun Initialization\": LecunNormal(),\n",
        "    \"Orthogonal Initialization\": Orthogonal()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to create different models"
      ],
      "metadata": {
        "id": "sCSBuVCdvVst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP Classifier"
      ],
      "metadata": {
        "id": "OteK4JIKvbBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq3uzqgf4L-B"
      },
      "outputs": [],
      "source": [
        "def create_mlp(input_shape, output_units, initializer, activation=\"softmax\"):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Flatten()(inputs)\n",
        "    x = Dense(256, activation='sigmoid', kernel_initializer=initializer)(x)\n",
        "    x = Dense(128, activation='tanh', kernel_initializer=initializer)(x)\n",
        "    outputs = Dense(output_units, activation=activation, kernel_initializer=initializer)(x)\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "wu1Kd0WNvc-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjBK5EZD4OCF"
      },
      "outputs": [],
      "source": [
        "def create_cnn(input_shape, output_units, initializer, activation=\"softmax\"):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', kernel_initializer=initializer)(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', kernel_initializer=initializer)(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu', kernel_initializer=initializer)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(output_units, activation=activation, kernel_initializer=initializer)(x)\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP Regressor"
      ],
      "metadata": {
        "id": "0KaN-lxsveH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scPNA-qj4Q43"
      },
      "outputs": [],
      "source": [
        "def create_regression_model(input_shape, initializer):\n",
        "    inputs = Input(shape=(input_shape,))\n",
        "    x = Normalization()(inputs)  # Apply Normalization layer\n",
        "    x = Dense(64, activation='relu', kernel_initializer=initializer)(x)\n",
        "    x = Dense(32, activation='relu', kernel_initializer=initializer)(x)\n",
        "    outputs = Dense(1, kernel_initializer=initializer)(x)\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to train a model given dataset and initializer"
      ],
      "metadata": {
        "id": "q_yFo74UvuAF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr2Qwxvv4Spa"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, dataset_name, initializer_name, model_type, epochs=50):\n",
        "    (x_train, y_train), (x_test, y_test) = load_dataset(dataset_name)\n",
        "\n",
        "    loss_fn = 'sparse_categorical_crossentropy' if model_type != 'Regression' else 'mse'\n",
        "    metrics = ['accuracy'] if model_type != 'Regression' else ['mae']\n",
        "    model.compile(optimizer='adam', loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    model_dir = f'models/{model_type}/{dataset_name}/{initializer_name}/'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(model_dir, 'best_model.keras')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=64, validation_data=(x_test, y_test), callbacks=callbacks, verbose=0)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return history.history, test_acc, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running experiments for all possible scenarios"
      ],
      "metadata": {
        "id": "nkvp2p2ovzpu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXReOanR4UEJ",
        "outputId": "9516f39b-9f29-4081-91c9-6b3b8c1dbf3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MLP on MNIST with Zero Initialization...\n",
            "MLP on MNIST with Zero Initialization: Test Accuracy = 0.1135\n",
            "Training MLP on MNIST with Random Initialization...\n",
            "MLP on MNIST with Random Initialization: Test Accuracy = 0.9800\n",
            "Training MLP on MNIST with Xavier Initialization...\n",
            "MLP on MNIST with Xavier Initialization: Test Accuracy = 0.9793\n",
            "Training MLP on MNIST with He Initialization...\n",
            "MLP on MNIST with He Initialization: Test Accuracy = 0.9772\n",
            "Training MLP on MNIST with Lecun Initialization...\n",
            "MLP on MNIST with Lecun Initialization: Test Accuracy = 0.9799\n",
            "Training MLP on MNIST with Orthogonal Initialization...\n",
            "MLP on MNIST with Orthogonal Initialization: Test Accuracy = 0.9798\n",
            "Training MLP on Fashion-MNIST with Zero Initialization...\n",
            "MLP on Fashion-MNIST with Zero Initialization: Test Accuracy = 0.1000\n",
            "Training MLP on Fashion-MNIST with Random Initialization...\n",
            "MLP on Fashion-MNIST with Random Initialization: Test Accuracy = 0.8864\n",
            "Training MLP on Fashion-MNIST with Xavier Initialization...\n"
          ]
        }
      ],
      "source": [
        "# Run experiments\n",
        "results = {}\n",
        "for model_type in [\"MLP\", \"CNN\", \"Regression\"]:\n",
        "    for dataset_name in [\"MNIST\", \"Fashion-MNIST\", \"CIFAR-10\", \"CIFAR-100\"] if model_type != \"Regression\" else [\"California Housing\"]:\n",
        "        output_units = 10 if dataset_name in [\"MNIST\", \"Fashion-MNIST\", \"CIFAR-10\"] else 100 if dataset_name == \"CIFAR-100\" else 1\n",
        "        activation = \"softmax\" if model_type != \"Regression\" else None\n",
        "\n",
        "        for initializer_name, initializer in initializers.items():\n",
        "            print(f\"Training {model_type} on {dataset_name} with {initializer_name}...\")\n",
        "\n",
        "            if model_type == \"MLP\":\n",
        "                model = create_mlp((28, 28) if \"MNIST\" in dataset_name else (32, 32, 3), output_units, initializer, activation)\n",
        "            elif model_type == \"CNN\":\n",
        "                model = create_cnn((28, 28, 1) if \"MNIST\" in dataset_name else (32, 32, 3), output_units, initializer, activation)\n",
        "            else:\n",
        "                model = create_regression_model(8, initializer)\n",
        "\n",
        "            history, test_acc, test_loss = train_model(model, dataset_name, initializer_name, model_type)\n",
        "            results[(model_type, dataset_name, initializer_name)] = {\n",
        "                \"history\": history,\n",
        "                \"test_acc\": test_acc,\n",
        "                \"test_loss\": test_loss\n",
        "            }\n",
        "            print(f\"{model_type} on {dataset_name} with {initializer_name}: Test Accuracy = {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame of results"
      ],
      "metadata": {
        "id": "ihn8TWMwv47z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzE6ho5V4XhF"
      },
      "outputs": [],
      "source": [
        "# Print results in table format\n",
        "df_results = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": model_type,\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Initializer\": initializer_name,\n",
        "        \"Validation Accuracy\": round(max(result[\"history\"][\"val_acc\"]), 4) if model_type != \"Regression\" else \"N/A\",\n",
        "        \"Test Accuracy\": round(result[\"test_acc\"], 4) if model_type != \"Regression\" else \"N/A\",\n",
        "        \"Regression MAE\": round(result[\"test_acc\"], 4) if model_type == \"Regression\" else \"N/A\",\n",
        "        \"Test Loss\": round(result[\"test_loss\"], 4),\n",
        "        \"Epochs\": len(result[\"history\"][\"val_loss\"])\n",
        "    }\n",
        "    for (model_type, dataset_name, initializer_name), result in results.items()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD14t8Q_4ZfU"
      },
      "outputs": [],
      "source": [
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing training curves (loss and accuracy)"
      ],
      "metadata": {
        "id": "A8YfZDFQv8OP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_3Z4_cW4qKF"
      },
      "outputs": [],
      "source": [
        "# Function to plot training curves\n",
        "def plot_training_curves(history, model_type, dataset_name, initializer_name):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Loss Curve - {model_type} on {dataset_name} ({initializer_name})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy curve if available\n",
        "    if 'accuracy' in history:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Accuracy Curve - {model_type} on {dataset_name} ({initializer_name})')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZvonXDb4xED"
      },
      "outputs": [],
      "source": [
        "for (model_type, dataset_name, initializer_name), result in results.items():\n",
        "    plot_training_curves(result[\"history\"], model_type, dataset_name, initializer_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}